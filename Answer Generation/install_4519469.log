Activating conda environment llava_2...
Checking ninja installation...
1.11.1.git.kitware.jobserver-1
Ninja is already installed and working correctly.
Setting environment variables to force ninja usage...
Testing ninja functionality...
[1/1] echo "Ninja is working"
Ninja is working
Ninja test completed, returning to project directory.
Installing flash-attn with no-build-isolation flag...
Collecting flash-attn==2.0.4
  Using cached flash_attn-2.0.4.tar.gz (4.2 MB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: torch in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from flash-attn==2.0.4) (2.1.2)
Requirement already satisfied: einops in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from flash-attn==2.0.4) (0.6.1)
Requirement already satisfied: packaging in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from flash-attn==2.0.4) (24.2)
Requirement already satisfied: ninja in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from flash-attn==2.0.4) (1.11.1.4)
Requirement already satisfied: filelock in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (3.18.0)
Requirement already satisfied: typing-extensions in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (4.13.0)
Requirement already satisfied: sympy in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (1.13.3)
Requirement already satisfied: networkx in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (3.4.2)
Requirement already satisfied: jinja2 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (3.1.6)
Requirement already satisfied: fsspec in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (2023.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from torch->flash-attn==2.0.4) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.0.4) (12.8.93)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.0.4) (3.0.2)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.0.4) (1.3.0)
Building wheels for collected packages: flash-attn
  Building wheel for flash-attn (setup.py): started
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): still running...
  Building wheel for flash-attn (setup.py): finished with status 'done'
  Created wheel for flash-attn: filename=flash_attn-2.0.4-cp310-cp310-linux_x86_64.whl size=117951722 sha256=be91ac85b84d6e9d05d60b1864d9f577e879e75efcb86e9b197ad656828c47a7
  Stored in directory: /home/ayliu2/.cache/pip/wheels/08/21/a3/4b5af3b331e945cfd495123deba2969ee5b8dd4c455137e02a
Successfully built flash-attn
Installing collected packages: flash-attn
Successfully installed flash-attn-2.0.4
Downloading LLaVA-Next model...
Requirement already satisfied: huggingface_hub in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (0.30.1)
Requirement already satisfied: filelock in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)
Requirement already satisfied: packaging>=20.9 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from huggingface_hub) (4.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.20)
Requirement already satisfied: certifi>=2017.4.17 in /home/ayliu2/miniconda3/envs/llava_2/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.1.31)
Running the demo...
[2025-04-07 11:27:06,225] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/ayliu2/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Job completed!
